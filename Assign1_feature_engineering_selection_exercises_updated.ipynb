{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9a4fb9e1",
   "metadata": {},
   "source": [
    "# Feature Engineering and Feature Selection Exercises\n",
    "This notebook contains hands-on exercises for feature engineering and feature selection using Python.\n",
    "Datasets used:\n",
    "- Titanic (from seaborn)\n",
    "- Iris (from sklearn)\n",
    "- Breast Cancer (from sklearn)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad0c1eb8",
   "metadata": {},
   "source": [
    "## Part 1 – Feature Engineering\n",
    "### Exercise 1: Create new features from existing ones\n",
    "**Dataset:** Titanic dataset (`seaborn.load_dataset('titanic')`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "82847e63",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "# Load Titanic dataset\n",
    "titanic = sns.load_dataset('titanic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd7ff6e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0. print the first few rows of the dataset \n",
    "# 1. Create FamilySize = sibsp + parch + 1\n",
    "# 2. Create IsAlone = 1 if FamilySize == 1 else 0\n",
    "# 3. Convert 'sex' column into numerical format (0=female, 1=male) (Hint: use map. for example: df['col'] = df['col'].map({'a': 1, 'b': 2}))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79ea3b7e",
   "metadata": {},
   "source": [
    "### Exercise 2: Handling missing values\n",
    "1. Identify columns with missing values\n",
    "2. Impute missing numerical values with the median\n",
    "3. Impute missing categorical values with the mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7efa22c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify missing values (hint: use isna().sum())\n",
    "# Impute missing values:\n",
    "#   1. missing numerical <- median: (hint: use select_dtypes(include=['number']).columns to select numerical columns. and then use fillna)\n",
    "#   2. missing categorical <- mode: (hint: user select_dtypes(include=['object', 'category']).columns to select categorical columns. and then use fillna)\n",
    "#   3. verify if there are still missing values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a30199d",
   "metadata": {},
   "source": [
    "### Exercise 3: Encoding categorical variables\n",
    "**Dataset:** Iris dataset (`sklearn.datasets.load_iris`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3abfc33",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "\n",
    "iris = load_iris(as_frame=True) # load data as a DataFrame, where (data, target) will be pandas DataFrames or Series\n",
    "iris_df = iris.data.copy()  # features only, without inbuilt 'target' column.\n",
    "iris_df['species'] = pd.Index(iris.target_names)[iris.target] # add 'species' column to iris_df using target names\n",
    "\n",
    "display(iris_df.head())  # Display the first few rows of the iris dataset\n",
    "display(\"Unique species names: \", iris_df['species'].unique())  # Display unique species names\n",
    "\n",
    "# --- Encode species names using One-Hot Encoding and Label Encoding ---\n",
    "    # 1. label encoding (hint: use LabelEncoder, and fit_transform)\n",
    "\n",
    "    # 2. (optional): want to see the mapping? uncomment the following code (remember to replace 'le' by your label encoder variable name)\n",
    "# id_to_name = {i: c for i, c in enumerate(le.classes_)}\n",
    "# display(id_to_name)\n",
    "\n",
    "    # 3. Onehot encoding \n",
    "        #  (Hint: use OneHotEncoder(sparse_output=False, handle_unknown='ignore'), \n",
    "        #  then fit_transform, \n",
    "        #  after that use et_feature_names_out(['species']).tolist() to get the one-hot encoded column names, \n",
    "        #  and then add the one-hot encoded columns to iris_df)\n",
    "\n",
    "\n",
    "# (optional): display the first few rows of the iris dataset. see any difference? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccc36aa6",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f1fdfa31",
   "metadata": {},
   "source": [
    "## Part 2 – Feature Selection\n",
    "### Exercise 4: Correlation-based selection\n",
    "**Dataset:** Continuous with the processed Titanic dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6d9f0e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Compute correlation matrix for numerical variables\n",
    "    # 1. select numerical columns: (HINT you have done this before in part 1))\n",
    "    \n",
    "    # 2. compute correlation matrix (Hint: use corr(numeric_only=True) to compute correlation matrix for numerical variables\n",
    "    \n",
    "    # 3. (optional) Plot matrix \n",
    "        # (HINT: uncomment the following lines to plot the correlation matrix.\n",
    "        #  Remember to import seaborn and matplotlib.pyplot and replace 'corr' by your correlation matrix variable name)\n",
    "# plt.figure(figsize=(9,7))\n",
    "# sns.heatmap(corr, annot=True, fmt=\".2f\", vmin=-1, vmax=1, square=True,\n",
    "#             linewidths=0.5, cbar_kws={\"label\": \"Pearson r\"})\n",
    "# plt.title(\"Correlation matrix Titanic Dataset\", fontsize=16)\n",
    "# plt.show()\n",
    "\n",
    "    # 4. Drop variables with high correlation (>0.85)\n",
    "thr = 0.85 \n",
    "keep_fea = []   # features we keep\n",
    "drop_fea = []   # features to drop\n",
    "\n",
    "        # (HINT: use for loop to iterate over the columns of the correlation matrix, \n",
    "        #   for example: for c in corr.columns: if any(corr.loc[c, k] > thr for k in keep_fea): ??_fea.append(c) else: ??_fea.append(c))\n",
    "\n",
    "\n",
    "#   5. (optional) if you want to create a reduced dataset with dropped features: (uncomment the following line)\n",
    "# titanic_reduced = titanic.drop(columns=drop) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b48bce9",
   "metadata": {},
   "source": [
    "### Exercise 5: Univariate feature selection\n",
    "**Dataset:** Breast Cancer dataset (`sklearn.datasets.load_breast_cancer`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "270aca4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "cancer = load_breast_cancer(as_frame=True)\n",
    "X = cancer.data\n",
    "y = cancer.target\n",
    "\n",
    "    # 1. Train/test split (HINT: use train_test_split from sklearn.model_selection)\n",
    "\n",
    "    # 2. Select top 5 features \n",
    "        # (hint: using SelectKBest(score_func=f_classif, k=5) , and then fit it to the training data. \n",
    "        #  After that, use get_support() to get a boolean mask of selected features, and then use it to get the feature names from X.columns)\n",
    "    \n",
    "    # 3. make training dataset based on the selected features: \n",
    "\n",
    "    # 4. (optional) Training a Logistic Regression model with selected features\n",
    "        # HINT: Since we have not talked about the algorithms, it is possible for you to use the code provided below:\n",
    "# clf_5 = LogisticRegression(max_iter=100, # max_iter is increased to ensure convergence\n",
    "#                          solver=\"liblinear\"  # use liblinear for small datasets\n",
    "#                          )\n",
    "# clf_5.fit(X_train_sel_5, y_train)  # remeber replace X_train_sel_5 with your training dataset based on selected features\n",
    "# y_pred = clf_5.predict(X_test_sel_5)  # remember to replace X_test_sel_5 with your test dataset based on selected features\n",
    "# print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9e10dbe",
   "metadata": {},
   "source": [
    "### Exercise 6: Recursive Feature Elimination (RFE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4016a35f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import RFE\n",
    "\n",
    "    # 1. Use RFE with logistic regression to select top 7 features \n",
    "        # (Hint: use RFE(estimator=clf_5, n_features_to_select=7, step=1), \n",
    "        # then fit it to the training data, \n",
    "        # and then use X.columns[rfe.support_].tolist() to get the selected features)\n",
    "    \n",
    "    # 2. Make training dataset based on the selected 7 features\n",
    "\n",
    "\n",
    "    # 3. Train new model on selected features and Compare accuracy with and without RFE. What the accuracy difference?\n",
    "        # (HINT, uncomment the following lines to train a new model and compare accuracy with the previous model)---\n",
    "# clf_7 = LogisticRegression(max_iter=100,\n",
    "#                            solver=\"liblinear\")\n",
    "# clf_7.fit(X_train_sel_7, y_train)\n",
    "# y_pred_7 = clf_7.predict(X_test_sel_7)\n",
    "# print(\"Accuracy:\", accuracy_score(y_test, y_pred_7))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e2bf663",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
